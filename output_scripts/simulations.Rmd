---
title: "Simulation Results"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide
    fig_width: 6
    fig_height: 4.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "figure/",
                      dev = 'png', fig.height = 4.5, fig.width = 6)
library(tidyverse)
library(Rcpp)
library(RcppArmadillo)


sourceCpp("../cpp_files/onlineblock.cpp")

theme_set(theme_classic())

set.seed(200)

```


```{r set_plot_parameters}
axis_nums <- 12
axis_text <- 14
leg_text <- 12

theme_update(axis.title = element_text(size = axis_text),
             axis.text = element_text(size = axis_nums),
             legend.text = element_text(size = leg_text))
```



This notebook will contain all simulation results in support of
the paper "Online Community Detection for Event Streams on Large 
Networks".


## Comparison to Aggregation of Windowed Data

We wish to simulate some data from a point process model of the form considered 
in this paper, and consider the results of community recovery methods based on forming
static adjacency matrices from binning the continuous time event data.

```{r simulate inhom Hawkes}
m <- 50
K <- 2
H <- 2
MuA <- array(0,c(K,K,H))
MuA[,,1] <- matrix(c(0.8,0.2,0.6,0.4),2,2)
MuA[,,2] <- matrix(c(0.4,0.7,0.2,0.7),2,2)

Mu <- matrix(c(0.6,0.2,0.3,0.5),K,K,byrow = TRUE)
B <- matrix(c(0.1,0.02,0.01,0.05),K,K,byrow = TRUE)
Pi <- matrix(c(0.4,0.6),1,2)
Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]))
Time <- 50

A <- list()
for(i in 1:m){
  # could sample these here with SBM structure...
  node_list <- c(1:m)
  edge <- sample(node_list[-i], 10) - 1
  edge <- sort(edge)
  A[[i]] <- edge
}

system.time(alltimes <- sampleBlockHak_nonhomo(Time,
                                                   A, Z, MuA, B, 
                                               window = 1, lam = 1))
```



```{r functions for binned data}
bin_fun <- function(events,m,max_Time,window_size){
  winds <- seq(from=window_size,to=max_Time,by= window_size)
  adj <- array(0, dim = c(m,m,length(winds)))
  for(i in 1:length(winds)){
    # get edges in each window and then updating the corresponding
    # entry in the adjacency matrix
    upper <- winds[i]
    lower <- upper-window_size
    wind_events <- events %>% 
      filter( Time < upper & Time > lower) %>%
      group_by(start,end) %>% tally()
    for(j in 1:nrow(wind_events)){
       row <- wind_events$start[j]+1
       col <- wind_events$end[j] + 1
       adj[row,col,i] <- wind_events$n[j] 
    }
  }
  return(adj)
}

test_events <- tibble(start = alltimes[,1],end = alltimes[,2],
                      Time = alltimes[,3])

out <- bin_fun(test_events,m,max_Time = 50,window_size = 0.5)
sum(out) - nrow(alltimes)

```



## Parmaeter and Community Recovery


We then wish to establish the performance of our proposed estimation
procedure in terms of community recovery in simulation examples. To
do this we simulate data under each of the proposed block structures
and evaluate community recovery for this simulated data,
comparing to the known true cluster assignments.

```{r sim_homogeneous_Poisson, cache=TRUE}
Time = 500 # total time
K <- 3 # number of communities
Mu <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),
             K,K,byrow = TRUE) # block level rates between communities
B_Pois <- matrix(0,K,K,byrow = TRUE) # block level excitation, none here
m <- 100 # number of nodes
Pi <- matrix(c(0.4,0.3,0.3),1,3) # proportion of nodes in each community
Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3])) # assignment vector
# of nodes to communities


A <- list() # store edge structure
for(i in 1:m){
  # could sample these here with SBM structure...
  node_list <- c(1:m)
  edge <- sample(node_list[-i], 40) - 1
  edge <- sort(edge)
  A[[i]] <- edge
}

# then store these events
pois_events <- sampleBlockHak(Time, A, Z, 
                              Mu, B_Pois,lam = 1)


```

```{r fit_homogeneous_Poisson, echo=TRUE, results='hide',cache=TRUE}
Pi = rep(1/K,K)
B = matrix(runif(K*K),K,K)
diag(B) = rnorm(K,mean = 1, sd = 0.1)
tau = matrix(runif(m*K),nrow=m,ncol=K)
tau = tau/rowSums(tau)
S = matrix(0,nrow = m,ncol = K)

dT <- 5
inter_T <- 5

results_online <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.5,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = TRUE)

```

We can compare the estimated and true rate matrix.

```{r compare_Poisson_matrix}
cat("Estimated Poisson rate matrix \n")
results_online$B
cat("True Poisson rate matrix \n")
Mu

```

## Informal Convergence of Proposed Method

```{r informal_convergence_plots}
iter = seq(dT,Time,dT)
av_elbo = results_online$AveELBO
ll = results_online$logL
elbo = as_tibble(cbind(iter,elbo = as.vector(av_elbo),ll= as.vector(ll)))
plot_1 <- ggplot(data = elbo, aes(iter,av_elbo)) + 
  geom_line() + 
  xlab("Time") + ylab("Average ELBO") 

plot_2 <- ggplot(data = elbo, aes(iter,ll)) + 
  geom_line()+ylab("Average Log likelihood") + xlab("Time")

plot_1
plot_2

```

```{r community_recovery_metrics}
est_Z = apply(results_online$tau,1,which.max)
cat("Estimated Adjusted Rand Index:",
    mclust::adjustedRandIndex(Z,est_Z), "\n",
    sep = " ")
cat("Estimated Normalised Mutual Information:",
    aricode::NMI(Z,est_Z), "\n",
    sep = " ")

```
```{r online_nmi_plot}
time_points = dim(results_online$early_tau)[3]-1
early_nmi = rep(0,time_points)
for(i in 1:time_points){
  est_Z = apply(results_online$early_tau[,,i],1,which.max)
  early_nmi[i]= aricode::NMI(Z,est_Z)
}

online_nmi = cbind(iters = seq(from = dT*inter_T,by = dT*inter_T,length.out = time_points),early_nmi)

as_tibble(online_nmi) %>% ggplot(aes(iters,early_nmi)) + geom_line() +
  ylab("Online NMI") + xlab("Time")
```

Do we want to run multiple simulations and get the average of the plot across 
those? I think possibly...

## Rate of Convergence Simulations


As discussed in our paper, we provide results on the convergence 
rate in terms of step size. 

```{r convergence_single_step_size}
inter_B = results_online$inter_B
norm_B = apply(inter_B,c(3),sum)

## need to adjust this also for sampling size
b_recov = cbind(iters = seq(dT,500,dT),diff = abs(sum(Mu)-norm_B)/K^2)

as_tibble(b_recov) %>% 
  ggplot(aes(iters,diff)) + geom_line() + xlab("Time") +
  ylab("Difference in Rate Parameters") + ggtitle(latex2exp::TeX( 
    'Step size of $\\alpha = 0.5$') )
```

We also wish to compare these results for multiple values of $\alpha$,
in support of the theoretical results presented in our paper.

```{r sims_multiple_step_sizes,results='hide',cache=TRUE}
results_alpha_05 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.5,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)

results_alpha_025 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.25,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)
results_alpha_075 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.75,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)

```



```{r convergence_data}
inter_B_05 <- results_alpha_05$inter_B
rm(results_alpha_05)

inter_B_025 <- results_alpha_025$inter_B
rm(results_alpha_025)

inter_B_075 <- results_alpha_075$inter_B
rm(results_alpha_075)


norm_B_05 = apply(inter_B_05,c(3),sum)
b_recov_05 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_05)/K^2)


norm_B_025 = apply(inter_B_025,c(3),sum)
b_recov_025 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_025)/K^2)


norm_B_075 = apply(inter_B_075,c(3),sum)
b_recov_075 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_075)/K^2)

```


```{r summarise data}

recov <- bind_rows(as_tibble(b_recov_025),
                   as_tibble(b_recov_05)) %>%
  bind_rows(as_tibble(b_recov_075))

num_iters <- length(seq(dT,Time,dT))

recov$step_size <- c(rep("alpha_025",num_iters),
                     rep("alpha_05",num_iters),
                     rep("alpha_075",num_iters))
# saveRDS(recov, file = "step_sims")

```

```{r plot_rate for steps}
ggplot(data = recov,aes(x=iters,y=diff,))+
  geom_line(aes(colour=step_size)) + 
  # scale_color_manual(name = "Step Size")
  #                    #values=c("blue","red","black")) +
  xlab("Time") + ylab("Difference in rate matrix") +
  scale_color_discrete(labels = unname(latex2exp::TeX(c("$\\alpha = \ 0.25",
                                                        "$\\alpha = \ 0.50",
                                                        "$\\alpha = \ 0.75"))) ) +
  theme(legend.title = element_blank())

# recov %>% filter(step_size == "alpha_025") %>%
#   ggplot(aes(iters,diff)) + geom_line()

```



```{r load_old_data, include=FALSE,eval=FALSE}
eta025 <- readRDS("../analysis_old/Simulations/eta025.RDS")
eta050 <- readRDS("../analysis_old/Simulations/eta050.RDS")
eta075 <- readRDS("../analysis_old/Simulations/eta075.RDS")


eta_df = rbind(eta025,eta050)
eta_df = rbind(eta_df,eta075)
step = c(rep("eta025",200),rep("eta050",200),rep("eta075",200))
eta_df$step <- step
eta_df$step = as.factor(eta_df$step)


ggplot(data = eta_df,aes(x=iters,y=diff,))+
  geom_line(aes(colour=step)) + 
  scale_color_manual(name = "Step Size",values=c("blue","red","black"))


```



## Average NMI

We want to repeat the above plot where we examine the
cluster recovery over time, now considering multiple simulated
datasets. To do this we will run this here and cache the results.

```{r average_nmi_sim, cache=TRUE, results='hide'}
n_sims <- 100

# then run the sims here
Time <- 500
dT <- 5
inter_T <- 5
inters <- Time/(dT*inter_T)
sim_nmi = matrix(nrow=n_sims,ncol=inters)


for(iter in 1:n_sims) {
  print(iter)
  Time = 500
  #dT = 10
  K <- 3
  Mu <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),K,K,byrow = TRUE)
  B_Pois <- matrix(0,K,K,byrow = TRUE)
  m <- 100
  Pi <- matrix(c(0.4,0.3,0.3),1,3)
  Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3]))
  
  A <- list()
  for(i in 1:m){
    # could sample these here with SBM structure...
    edge_list <- c(1:m)
    edge <- sample(edge_list[-i], 40) - 1
    edge <- sort(edge)
    A[[i]] <- edge
  }
  
  system.time(alltimes <- sampleBlockHak(Time, A, Z, Mu, B_Pois, lam = 1))
  
  
  
  Pi = rep(1/K,K)
  B = matrix(runif(K*K),K,K)
  diag(B) = rnorm(3,mean = 1, sd = 0.1)
  tau = matrix(runif(m*K),nrow=m,ncol=K)
  tau = tau/rowSums(tau)
  S = matrix(0,nrow = m,ncol = K)
  results_online <- estimate_Poisson(full_data = alltimes,
                                     A,m,K,Time,dT,
                                     step_size = 0.5,
                                     B,
                                     tau,Pi,S,inter_T,is_elbo = FALSE)
  #print(results_online$Pi)
  time_points = dim(results_online$early_tau)[3]-1
  early_nmi = rep(0,time_points)
  for(i in 1:time_points){
    est_Z = apply(results_online$early_tau[,,i],1,which.max)
    early_nmi[i]= aricode::NMI(Z,est_Z)
  }
  
  #online_nmi = cbind(windows = seq(50,500,50),early_nmi)
  online_nmi = cbind(iters = seq(from = dT*inter_T,
                                 by = dT*inter_T,
                                 length.out = time_points),early_nmi)
  sim_nmi[iter,] = early_nmi
}


```


```{r average_nmi_plot}
new_sim_nmi = t(sim_nmi)
colnames(new_sim_nmi) = paste("Sim",c(1:n_sims),sep="")

nmi_online = as_tibble(new_sim_nmi)
nmi_online$Windows = seq(dT*inter_T,Time,dT*inter_T)


nmi_online %>% pivot_longer(
  cols = starts_with("Sim")) %>%
  group_by(Windows) %>%
  summarise(Ave = mean(value),sdev = sd(value)) %>%
  mutate(se = sdev/sqrt(50)) %>%
  ggplot(aes(Windows,Ave)) + geom_line() + geom_point() +
  geom_errorbar(aes(ymin=Ave-se, ymax=Ave+se), width=.1,color="blue") +
  ylab("Average NMI") + xlab("Time")

```



## Intensity Plots