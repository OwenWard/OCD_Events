---
title: "Simulation Results"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
    fig_width: 6
    fig_height: 4.5
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "figure/",
                      dev = 'png', fig.height = 4.5, fig.width = 6)
library(tidyverse)
library(Rcpp)
library(RcppArmadillo)


sourceCpp("../cpp_files/onlineblock.cpp")

theme_set(theme_classic())

set.seed(200)

```


```{r set_plot_parameters}
axis_nums <- 12
axis_text <- 14
leg_text <- 12

theme_update(axis.title = element_text(size = axis_text),
             axis.text = element_text(size = axis_nums),
             legend.text = element_text(size = leg_text))
```



This notebook will contain all simulation results in support of
the paper "Online Community Detection for Event Streams on Large 
Networks".


## Comparison to Aggregation of Windowed Data

We wish to simulate some data from a point process model of the form considered 
in this paper, and consider the results of community recovery methods based on forming
static adjacency matrices from binning the continuous time event data.


We consider two methods for aggregation of the data. 

1. Constructing a count matrix, with each entry describing all directed interactions
between two nodes, and performing spectral clustering on this count matrix.
2. We follow [Pensky and Zhang](https://projecteuclid.org/euclid.ejs/1550286096),
where we construct an estimator matrix described there and perform spectral clustering on this matrix. 

For each, we compare the estimated community structure using spectral clustering
with a known number of communities to the results obtained from our procedure.
We consider a window size of $dT=0.5,1,5$ for each model, where this window size
is used to aggregate the data for the binned estimator and also the batch size in 
our data processing step.

### Block Poisson

We first simulate data from a SBM structure with a homogeneous Poisson
rate between node pairs, with each node interaction with a fixed number of 
nodes. As the number of nodes increase and these interactions become sparse,
we see that comparison methods which aggregate data are unable to recover the
community structure.



```{r fig-agg-rank-pois-dT05}
all_results <- readRDS("../Sims/output_sims/agg_comp_pois_dT0.5.RDS")

n_sims <- 100
m_values <- c(50,100,250,500)

pois_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width=10) + 
  scale_x_continuous(breaks = m_values) + ggtitle("dT=0.5")

pois_agg

```


```{r fig-agg-rank-pois-dT1}

all_results <- readRDS("../Sims/output_sims/agg_comp_pois_dT1.RDS")
pois_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width=10) + 
  scale_x_continuous(breaks = m_values) + ggtitle("dT=1")

pois_agg
```


```{r fig-agg-rank-pois-dT5}

all_results <- readRDS("../Sims/output_sims/agg_comp_pois_dT5.RDS")
pois_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width=10) + 
  scale_x_continuous(breaks = m_values) + ggtitle("dT=5")

pois_agg

```



### Inhomogeneous Hawkes

We can also repeat this with an inhomogeneous Hawkes structure. Here aggregation
methods are unable to recover this community structure.

Note, our method requires a flexible enough non-parametric function, i.e a large
value of H.




```{r fig-agg-rank-inhawkes, message=FALSE,warning=FALSE}

all_results <- readRDS("../Sims/output_sims/agg_comp_inhom_hawk_dT0.5H_4.RDS")

inhaw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=0.5,H=4,true H = 4")

inhaw_agg

all_results <- readRDS("../Sims/output_sims/agg_comp_inhom_hawk_dT1H_4.RDS")

inhaw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=1,H=4,true H = 4")

inhaw_agg


all_results <- readRDS("../Sims/output_sims/agg_comp_inhom_hawk_dT5H_4.RDS")

inhaw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=5,H=4,true H = 4")

inhaw_agg


all_results <- readRDS("../Sims/output_sims/agg_comp_inhom_hawk_true_H_2_dT1H_2.RDS")

inhaw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=1,H=4,true H = 2")

inhaw_agg

```

Here the estimation procedure is somewhat more complex, depending on the number
of basis window functions chosen and the length of the corresponding window
of these basis functions.


### Hawkes

```{r hawkes-sim-dT-05,message=FALSE}
all_results <- readRDS("../Sims/output_sims/agg_comp_hawk_dT0.5.RDS")

haw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=0.5")

haw_agg

```

```{r hawkes-sim-dT-1}

all_results <- readRDS("../Sims/output_sims/agg_comp_hawk_dT1.RDS")

haw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=1")

haw_agg

```


```{r hawkes-sim-dT-5}
all_results <- readRDS("../Sims/output_sims/agg_comp_hawk_dT5.RDS")

haw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values) +
  ggtitle("dT=5")

haw_agg

```








This recovers the clusters with the correct window size...., which the binning
methods do not, even with the correct window size.


```{r comp-estimators, include=TRUE,echo=TRUE,eval=FALSE}
# using sum of all events
aricode::ARI(fcd::spectral.clustering(out[,,dim(out)[3]]),Z) 
# using only events in final time window
sum_adj <- apply(out, c(1,2), sum)/dim(out)[3]
aricode::ARI(fcd::spectral.clustering(sum_adj,K=2),Z)
# our method
aricode::ARI(Z,est_Z)

### considering just binary (so at least one event)
out_bin <- out
out_bin <- apply(out, c(2,3), function(x) as.numeric(x>0))
aricode::ARI(fcd::spectral.clustering(out_bin[,,dim(out_bin)[3]]),Z) 
# using only events in final time window
sum_adj_bin <- apply(out_bin, c(1,2), sum)/dim(out_bin)[3]
aricode::ARI(fcd::spectral.clustering(sum_adj,K=2),Z)
# our method
aricode::ARI(Z,est_Z)

```



## Parmaeter and Community Recovery


We then wish to establish the performance of our proposed estimation
procedure in terms of community recovery in simulation examples. To
do this we simulate data under each of the proposed block structures
and evaluate community recovery for this simulated data,
comparing to the known true cluster assignments.

```{r sim_homogeneous_Poisson, cache=TRUE}
Time = 500 # total time
K <- 3 # number of communities
Mu <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),
             K,K,byrow = TRUE) # block level rates between communities
B_Pois <- matrix(0,K,K,byrow = TRUE) # block level excitation, none here
m <- 100 # number of nodes
Pi <- matrix(c(0.4,0.3,0.3),1,3) # proportion of nodes in each community
Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3])) # assignment vector
# of nodes to communities


A <- list() # store edge structure
for(i in 1:m){
  # could sample these here with SBM structure...
  node_list <- c(1:m)
  edge <- sample(node_list[-i], 40) - 1
  edge <- sort(edge)
  A[[i]] <- edge
}

# then store these events
pois_events <- sampleBlockHak(Time, A, Z, 
                              Mu, B_Pois,lam = 1)


```

```{r fit_homogeneous_Poisson, echo=TRUE, results='hide',cache=TRUE}
Pi = rep(1/K,K)
B = matrix(runif(K*K),K,K)
diag(B) = rnorm(K,mean = 1, sd = 0.1)
tau = matrix(runif(m*K),nrow=m,ncol=K)
tau = tau/rowSums(tau)
S = matrix(0,nrow = m,ncol = K)

dT <- 5
inter_T <- 5

results_online <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.5,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = TRUE)

```

We can compare the estimated and true rate matrix.

```{r compare_Poisson_matrix}
cat("Estimated Poisson rate matrix \n")
results_online$B
cat("True Poisson rate matrix \n")
Mu

```

## Informal Convergence of Proposed Method

```{r informal_convergence_plots}
iter = seq(dT,Time,dT)
av_elbo = results_online$AveELBO
ll = results_online$logL
elbo = as_tibble(cbind(iter,elbo = as.vector(av_elbo),ll= as.vector(ll)))
plot_1 <- ggplot(data = elbo, aes(iter,av_elbo)) + 
  geom_line() + 
  xlab("Time") + ylab("Average ELBO") 

plot_2 <- ggplot(data = elbo, aes(iter,ll)) + 
  geom_line()+ylab("Average Log likelihood") + xlab("Time")

plot_1
plot_2

```

```{r community_recovery_metrics}
est_Z = apply(results_online$tau,1,which.max)
cat("Estimated Adjusted Rand Index:",
    mclust::adjustedRandIndex(Z,est_Z), "\n",
    sep = " ")
cat("Estimated Normalised Mutual Information:",
    aricode::NMI(Z,est_Z), "\n",
    sep = " ")

```
```{r online_nmi_plot}
time_points = dim(results_online$early_tau)[3]-1
early_nmi = rep(0,time_points)
for(i in 1:time_points){
  est_Z = apply(results_online$early_tau[,,i],1,which.max)
  early_nmi[i]= aricode::NMI(Z,est_Z)
}

online_nmi = cbind(iters = seq(from = dT*inter_T,by = dT*inter_T,length.out = time_points),early_nmi)

as_tibble(online_nmi) %>% ggplot(aes(iters,early_nmi)) + geom_line() +
  ylab("Online NMI") + xlab("Time")
```

Do we want to run multiple simulations and get the average of the plot across 
those? I think possibly...

## Rate of Convergence Simulations


As discussed in our paper, we provide results on the convergence 
rate in terms of step size. 

```{r convergence_single_step_size}
inter_B = results_online$inter_B
norm_B = apply(inter_B,c(3),sum)

## need to adjust this also for sampling size
b_recov = cbind(iters = seq(dT,500,dT),diff = abs(sum(Mu)-norm_B)/K^2)

as_tibble(b_recov) %>% 
  ggplot(aes(iters,diff)) + geom_line() + xlab("Time") +
  ylab("Difference in Rate Parameters") + ggtitle(latex2exp::TeX( 
    'Step size of $\\alpha = 0.5$') )
```

We also wish to compare these results for multiple values of $\alpha$,
in support of the theoretical results presented in our paper.

```{r sims_multiple_step_sizes,results='hide',cache=TRUE}
results_alpha_05 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.5,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)

results_alpha_025 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.25,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)
results_alpha_075 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.75,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)

```



```{r convergence_data}
inter_B_05 <- results_alpha_05$inter_B
rm(results_alpha_05)

inter_B_025 <- results_alpha_025$inter_B
rm(results_alpha_025)

inter_B_075 <- results_alpha_075$inter_B
rm(results_alpha_075)


norm_B_05 = apply(inter_B_05,c(3),sum)
b_recov_05 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_05)/K^2)


norm_B_025 = apply(inter_B_025,c(3),sum)
b_recov_025 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_025)/K^2)


norm_B_075 = apply(inter_B_075,c(3),sum)
b_recov_075 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_075)/K^2)

```


```{r summarise data}

recov <- bind_rows(as_tibble(b_recov_025),
                   as_tibble(b_recov_05)) %>%
  bind_rows(as_tibble(b_recov_075))

num_iters <- length(seq(dT,Time,dT))

recov$step_size <- c(rep("alpha_025",num_iters),
                     rep("alpha_05",num_iters),
                     rep("alpha_075",num_iters))
# saveRDS(recov, file = "step_sims")

```

```{r plot_rate for steps}
ggplot(data = recov,aes(x=iters,y=diff,))+
  geom_line(aes(colour=step_size)) + 
  # scale_color_manual(name = "Step Size")
  #                    #values=c("blue","red","black")) +
  xlab("Time") + ylab("Difference in rate matrix") +
  scale_color_discrete(labels = unname(latex2exp::TeX(c("$\\alpha = \ 0.25",
                                                        "$\\alpha = \ 0.50",
                                                        "$\\alpha = \ 0.75"))) ) +
  theme(legend.title = element_blank())

# recov %>% filter(step_size == "alpha_025") %>%
#   ggplot(aes(iters,diff)) + geom_line()

```



```{r load_old_data, include=FALSE,eval=FALSE}
eta025 <- readRDS("../analysis_old/Simulations/eta025.RDS")
eta050 <- readRDS("../analysis_old/Simulations/eta050.RDS")
eta075 <- readRDS("../analysis_old/Simulations/eta075.RDS")


eta_df = rbind(eta025,eta050)
eta_df = rbind(eta_df,eta075)
step = c(rep("eta025",200),rep("eta050",200),rep("eta075",200))
eta_df$step <- step
eta_df$step = as.factor(eta_df$step)


ggplot(data = eta_df,aes(x=iters,y=diff,))+
  geom_line(aes(colour=step)) + 
  scale_color_manual(name = "Step Size",values=c("blue","red","black"))


```



## Average NMI

We want to repeat the above plot where we examine the
cluster recovery over time, now considering multiple simulated
datasets. To do this we will run this here and cache the results.

```{r average_nmi_sim, cache=TRUE, results='hide'}
n_sims <- 100

# then run the sims here
Time <- 500
dT <- 5
inter_T <- 5
inters <- Time/(dT*inter_T)
sim_nmi = matrix(nrow=n_sims,ncol=inters)


for(iter in 1:n_sims) {
  print(iter)
  Time = 500
  #dT = 10
  K <- 3
  Mu <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),K,K,byrow = TRUE)
  B_Pois <- matrix(0,K,K,byrow = TRUE)
  m <- 100
  Pi <- matrix(c(0.4,0.3,0.3),1,3)
  Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3]))
  
  A <- list()
  for(i in 1:m){
    # could sample these here with SBM structure...
    edge_list <- c(1:m)
    edge <- sample(edge_list[-i], 40) - 1
    edge <- sort(edge)
    A[[i]] <- edge
  }
  
  system.time(alltimes <- sampleBlockHak(Time, A, Z, Mu, B_Pois, lam = 1))
  
  
  
  Pi = rep(1/K,K)
  B = matrix(runif(K*K),K,K)
  diag(B) = rnorm(3,mean = 1, sd = 0.1)
  tau = matrix(runif(m*K),nrow=m,ncol=K)
  tau = tau/rowSums(tau)
  S = matrix(0,nrow = m,ncol = K)
  results_online <- estimate_Poisson(full_data = alltimes,
                                     A,m,K,Time,dT,
                                     step_size = 0.5,
                                     B,
                                     tau,Pi,S,inter_T,is_elbo = FALSE)
  #print(results_online$Pi)
  time_points = dim(results_online$early_tau)[3]-1
  early_nmi = rep(0,time_points)
  for(i in 1:time_points){
    est_Z = apply(results_online$early_tau[,,i],1,which.max)
    early_nmi[i]= aricode::NMI(Z,est_Z)
  }
  
  #online_nmi = cbind(windows = seq(50,500,50),early_nmi)
  online_nmi = cbind(iters = seq(from = dT*inter_T,
                                 by = dT*inter_T,
                                 length.out = time_points),early_nmi)
  sim_nmi[iter,] = early_nmi
}


```


```{r average_nmi_plot}
new_sim_nmi = t(sim_nmi)
colnames(new_sim_nmi) = paste("Sim",c(1:n_sims),sep="")

nmi_online = as_tibble(new_sim_nmi)
nmi_online$Windows = seq(dT*inter_T,Time,dT*inter_T)


nmi_online %>% pivot_longer(
  cols = starts_with("Sim")) %>%
  group_by(Windows) %>%
  summarise(Ave = mean(value),sdev = sd(value)) %>%
  mutate(se = sdev/sqrt(50)) %>%
  ggplot(aes(Windows,Ave)) + geom_line() + geom_point() +
  geom_errorbar(aes(ymin=Ave-se, ymax=Ave+se), width=.1,color="blue") +
  ylab("Average NMI") + xlab("Time")

```



## Intensity Plots



## Link Prediction

We wish to demonstrate the advantage of this model in terms of link 
prediction also. To do this, we consider online event prediction,
where we predict the number of events in a short future time window,
update the model using the data from that period and then predict for 
the following time period. In this way we can track the evolution
of the predictions from our model as successive events have been analysed,
with the hope that predictions will improve over time.


We first simulate events from a homogeneous Poisson process.

```{r simulate data for link prediction homog Poisson}
Time <- 200
dT <- 10
inter_T <- 1
K <- 3
B_true <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),K,K,byrow = TRUE)
B_Pois <- matrix(0,K,K,byrow = TRUE)
m <- 100
Pi <- matrix(c(0.4,0.3,0.3),1,3)
Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3]))

A <- list()
for(i in 1:m){
  # could sample these here with SBM structure...
  edge_list <- c(1:m)
  edge <- sample(edge_list[-i], 40) - 1
  edge <- sort(edge)
  A[[i]] <- edge
}

system.time(alltimes <- sampleBlockHak(Time, A, Z, B_true, B_Pois, lam = 1))
```
To perform link prediction, we want to first estimate the initial 
model on the first half of this data, then iterate over the second half
of events, in several increments, each time predicting for the next increment.

We can do this using only a single increment of the data by
passing the previous estimates in as the initial estimates for each stage.


```{r initial fit link prediction homog Poisson}
train_time <- 100

training_data <- alltimes[alltimes[,3]<= train_time,]
test_data <- alltimes[alltimes[,3]> train_time,]

## fit initial model on training data
B <- matrix(rexp(K*K),K,K)
tau <- matrix(runif(m*K),nrow=m,ncol=K)
tau <- tau/rowSums(tau)
S <- matrix(0,nrow = m,ncol = K)
Pi <- rep(1/K,K)

training_fit <- estimate_Poisson(training_data,A,m,K,T = train_time,
                                dT = 1, step_size = 0.5,
                                B,tau,Pi,S,inter_T)

# pass these in and repeatedly update them
S_start <- training_fit$S
tau_start <- training_fit$tau
B_start <- training_fit$B
Pi_start <- training_fit$Pi


```


Link prediction is relatively easy for this Poisson model.

```{r link pred homog Poisson,results='hide',message=FALSE,warning=FALSE}
time_windows <- seq(train_time,Time, by = 5)

test_data <- as_tibble(test_data)
colnames(test_data) <- c("send","rec","Time")

overall_pred <- tibble()


for(i in 1:(length(time_windows)-1)) {
  start_time <- time_windows[i]
  end_time <- time_windows[i+1]
  
  training_data <- as_tibble(training_data)
  colnames(training_data) <- c("send","rec","Time")
  
  training_counts <- training_data %>%
    group_by(send,rec) %>% tally()
  
  data_batch <- test_data %>% 
    filter(Time > start_time & Time < end_time)
  
  ### link predict for this data ###
  curr_z <- apply(tau_start,1,which.max)
  z_tibble <- tibble(id = 1:m-1, z = curr_z)
  
  train_pred <- training_counts %>% 
    left_join(z_tibble, by = c("send"="id")) %>%
    left_join(z_tibble, by = c("rec" = "id")) %>%
    rowwise() %>%
    mutate(est_int = B_start[z.x,z.y]) %>%
    mutate(pred_count = est_int*(end_time-start_time))
  
  true_counts <- data_batch %>%
    group_by(send,rec) %>%
    tally()
  
  difference <- train_pred %>% 
    full_join(true_counts, by = c("send","rec")) %>% 
    mutate(across(starts_with("n."),~replace_na(.x,0))) %>%
    mutate(diff = n.y-pred_count) %>%
    select(send,rec,pred_count,n.y,diff)
  
  current_result <- difference %>% ungroup() %>%
    summarise(norm = sqrt(sum(diff^2)))
  
  current_result$end <- end_time
  overall_pred <- bind_rows(overall_pred,current_result)
  
  ### then update the model params ####
  current_fit <- estimate_Poisson(as.matrix(data_batch),A,m,K,T = end_time,
                                  dT = 0.01, step_size = 0.5,
                                  B = B_start,tau = tau_start,
                                  Pi = Pi_start,
                                  S = S_start, inter_T)
  
  # update these params #
  S_start <- current_fit$S
  tau_start <- current_fit$tau
  B_start <- current_fit$B
  Pi_start <- current_fit$Pi
  
  ## update the training data
  training_data <- alltimes[alltimes[,3]<= end_time,]
  

  
  
}

```


```{r fig-event-pred-homog-poisson}

overall_pred %>% ggplot(aes(end,norm)) + geom_line()

```




## Supplementary Plots

Here we recreate the plots from the supplementary material
also, using a Hawkes process.


### Hawkes covergence


```{r hawkes-sim-data}
Time <- 200
dT <- 1.25
inter_T <- 5
K <- 3
Mu_H <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.2,0.75),K,K,byrow = TRUE)
B_H <- matrix(c(0.5,0.1,0.3,0.4,0.4,0.4,0.2,0.6,0.2),K,K,byrow = TRUE)
m <- 100
Pi <- matrix(c(0.4,0.3,0.3),1,3)
Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3]))

A <- list()
for(i in 1:m){
  edge <- sample(m, 15) - 1
  edge <- sort(edge)
  A[[i]] <- edge
}

system.time(alltimes_hawkes <- sampleBlockHak(Time, A, Z, Mu_H, B_H, lam = 1))



```

Recovery in this setting is possible, but unsurprisingly more difficult.

```{r hawkes-sim-fit, results='hide'}


Pi = rep(1/K,K)
Mu = matrix(runif(K*K),K,K)
B = matrix(runif(K*K),K,K)
#diag(B) = rnorm(3,mean = 1, sd = 0.1)
tau = matrix(runif(m*K),nrow=m,ncol=K)
tau = tau/rowSums(tau)
S = matrix(0,nrow = m,ncol = K)


results_hawkes_sim <- online_estimator_eff_revised(alltimes_hawkes,
                                                   A, m, K, Time, dT=1,
                                                   lam = 1, B, Mu,
                                                   tau,inter_T = 10,
                                                   is_elbo = T)

```

Plot the ELBO
```{r hawkes-elbo}
# only makes sense if is_elbo = T above
dT = 1
iter = seq(dT,Time,dT)
av_elbo = results_hawkes_sim$elbo
ll = results_hawkes_sim$logL
elbo = as_tibble(cbind(iter,elbo = as.vector(av_elbo),ll= as.vector(ll)))
ggplot(data = elbo, aes(iter,av_elbo)) + geom_line() + 
  xlab("Time") + ylab("Average ELBO") +      
  theme(axis.title=element_text(size=14))

```



```{r hawkes-clust-result}
results_hawkes_sim$Mu
Mu_H

results_hawkes_sim$B
B_H

mclust::adjustedRandIndex(Z,apply(results_hawkes_sim$tau,1,which.max))

```

We can also inspect the community recovery over time for this model.

```{r hawkes-online-nmi}
inter_T <- 10
time_points = dim(results_hawkes_sim$early_tau)[3]-1
early_nmi = rep(0,time_points)
for(i in 1:time_points){
  est_Z = apply(results_hawkes_sim$early_tau[,,i],1,which.max)
  early_nmi[i]= aricode::NMI(Z,est_Z)
}

online_nmi = cbind(iters = seq(from = inter_T,by = inter_T,length.out = time_points),early_nmi)

as_tibble(online_nmi) %>% ggplot(aes(iters,early_nmi)) + geom_line() +
  ylab("Online NMI") + xlab("Time")+
  theme(axis.title=element_text(size=14))

```

```{r fig-par-convergence-hawkes}
inter_B = results_hawkes_sim$inter_B
norm_B = apply(inter_B,c(3),sum)

inter_mu = results_hawkes_sim$inter_mu
norm_mu = apply(inter_mu,c(3),sum)

## need to adjust this also for sampling size
dT = 1
b_recov = cbind(iters = seq(dT,Time,dT),diff_B = abs(sum(B_H)-norm_B)/K^2)
#plot(b_recov[,1],b_recov[,2],type="l")
mu_recov = cbind(iters = seq(dT,Time,dT),
                diff_mu = abs(sum(Mu_H)-norm_mu)/K^2 )

pars <- c(rep("b",Time),rep("mu",Time))
recov <- rbind(b_recov,mu_recov)
recov <- as_tibble(recov)
recov$step <- pars
recov$step <- as.factor(recov$step)

as_tibble(recov) %>% 
  ggplot(aes(iters,diff_B)) + geom_line(aes(color=step)) +
  xlab("Time Windows") +
  #ylab("Difference in Rate Parameters") +
  ylab("Difference") +
  scale_color_manual(name = "Parameter",values=c("blue","red"))+
  #geom_line(aes(iters,diff_mu),col = "red") +
  theme(axis.title=element_text(size=14),legend.position = "right")

```

### Online NMI, Hawkes 


```{r hawkes_sims, cache=TRUE,results='hide'}
n_sims <- 50


results <- tibble()

for(iter in 1:n_sims) {
  print(iter)
  ## sample data
  Time <- 100
  dT <- 1
  inter_T <- 5
  K <- 3
  Mu_H <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.2,0.75),K,K,byrow = TRUE)
  B_H <- matrix(c(0.5,0.1,0.3,0.4,0.4,0.4,0.2,0.6,0.2),K,K,byrow = TRUE)
  m <- 100
  Pi <- matrix(c(0.4,0.3,0.3),1,3)
  Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3]))
  
  A <- list()
  for(i in 1:m){
    edge <- sample(m, 40) - 1
    edge <- sort(edge)
    A[[i]] <- edge
  }
  
  alltimes_hawkes <- sampleBlockHak(Time, A, Z, Mu_H, B_H, lam = 1)
  
  ### never estimate the model here
  
  Pi = rep(1/K,K)
  Mu = matrix(runif(K*K),K,K)
  B = matrix(runif(K*K),K,K)
  #diag(B) = rnorm(3,mean = 1, sd = 0.1)
  tau = matrix(runif(m*K),nrow=m,ncol=K)
  tau = tau/rowSums(tau)
  S = matrix(0,nrow = m,ncol = K)
  
  
  results_hawkes_sim <- online_estimator_eff_revised(alltimes_hawkes,
                                                     A, m, K, Time, dT,
                                                     lam = 1, B, Mu,
                                                     tau,inter_T,
                                                     is_elbo = F)
  
  time_points = dim(results_hawkes_sim$early_tau)[3]-1
  early_nmi = rep(0,time_points)
  for(i in 1:time_points){
    est_Z = apply(results_hawkes_sim$early_tau[,,i],1,which.max)
    early_nmi[i]= aricode::NMI(Z,est_Z)
  }
  
  #online_nmi = cbind(windows = seq(50,500,50),early_nmi)
  online_nmi <- as_tibble(cbind(iters = seq(from = dT*inter_T,
                                 by = dT*inter_T,
                                 length.out = time_points),early_nmi))
  online_nmi$sim_num <- iter
  #results[iter, ] <- early_nmi
  results <- bind_rows(results,online_nmi)
  
}


```

```{r fig-online-nmi-hawkes}
results %>%
group_by(iters) %>%
  summarise(Ave = mean(early_nmi),sdev = sd(early_nmi)) %>%
  mutate(se = sdev/sqrt(n_sims)) %>%
  ggplot(aes(iters,Ave)) + geom_line() + geom_point() +
  geom_errorbar(aes(ymin=Ave-se, ymax=Ave+se), width=.1,color="blue") +
  ylab("Average NMI") + xlab("Time")

```

