---
title: "Simulation Results"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    code_folding: hide
    fig_width: 6
    fig_height: 4.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "figure/",
                      dev = 'png', fig.height = 4.5, fig.width = 6)
library(tidyverse)
library(Rcpp)
library(RcppArmadillo)


sourceCpp("../cpp_files/onlineblock.cpp")

theme_set(theme_classic())

set.seed(200)

```


```{r set_plot_parameters}
axis_nums <- 12
axis_text <- 14
leg_text <- 12

theme_update(axis.title = element_text(size = axis_text),
             axis.text = element_text(size = axis_nums),
             legend.text = element_text(size = leg_text))
```



This notebook will contain all simulation results in support of
the paper "Online Community Detection for Event Streams on Large 
Networks".


## Comparison to Aggregation of Windowed Data

We wish to simulate some data from a point process model of the form considered 
in this paper, and consider the results of community recovery methods based on forming
static adjacency matrices from binning the continuous time event data.


We consider two methods for aggregation of the data. 

1. Constructing a count matrix, with each entry describing all directed interactions
between two nodes, and performing spectral clustering on this count matrix.
2. We follow [Pensky and Zhang](https://projecteuclid.org/euclid.ejs/1550286096),
where we construct an estimator matrix described there and perform spectral clustering on this matrix. 

For each, we compare the estimated community structure using spectral clustering
with a known number of communities to the results obtained from our procedure.

### Block Poisson

We first simulate data from a SBM structure with a homogeneous Poisson
rate between node pairs, with each node interaction with a fixed number of 
nodes. As the number of nodes increase and these interactions become sparse,
we see that comparison methods which aggregate data are unable to recover the
community structure.

```{r functions for binning}
source("C:/Users/owenw/Documents/Github_Local/Spectral_DSBM/R/pensky_fcns.R")


bin_fun <- function(events,m,max_Time,window_size){
  winds <- seq(from=window_size,to=max_Time,by= window_size)
  adj <- array(0, dim = c(m,m,length(winds)))
  for(i in 1:length(winds)){
    # get edges in each window and then updating the corresponding
    # entry in the adjacency matrix
    upper <- winds[i]
    lower <- upper-window_size
    wind_events <- events %>% 
      filter( Time < upper & Time > lower) %>%
      group_by(start,end) %>% tally()
    for(j in 1:nrow(wind_events)){
      row <- wind_events$start[j]+1
      col <- wind_events$end[j] + 1
      adj[row,col,i] <- wind_events$n[j] 
    }
  }
  return(adj)
}
```



```{r poisson-aggregate-comparison, eval=FALSE,include=FALSE ,results = 'hide', cache=TRUE,message=FALSE,warning=FALSE}
n_sims <- 20
m_values <- c(50,100,250,500)
no_methods <- 3 #ours, sum all, pz_estimator


l <- 2
m <- 2 # need m =l for final time point
m0 <- 1
l0 <- 2


all_results <- list(rep(tibble(),length(m_values)))
#all_results <- tibble()

for(i in seq_along(m_values)) {
  m_nodes <- m_values[i]
  #results <- matrix(NA, nrow = n_sims,ncol = no_methods )
  # then each sim
  results <- matrix(NA, nrow = n_sims,ncol = no_methods+2 )
  colnames(results) <- c("SC_Sum","OCD","SC_PZ","nodes","sim")
  results <- as_tibble(results)
  for(j in 1:n_sims) {
    print(j)
    K <- 2
    # H <- 2
    # MuA <- array(0,c(K,K,H))
    # MuA[,,1] <- matrix(c(0.8,0.2,0.6,0.4),2,2)
    # MuA[,,2] <- matrix(c(0.4,0.7,0.2,0.7),2,2)
    
    Mu <- matrix(c(0.6,0.2,0.3,0.5),K,K,byrow = TRUE)
    B <- matrix(0,K,K,byrow = TRUE)
    Pi <- matrix(c(0.4,0.6),1,2)
    Z <- c(rep(0,m_nodes*Pi[1]),rep(1,m_nodes*Pi[2]))
    Time <- 100
    
    A <- list()
    for(k in 1:m_nodes){
      # could sample these here with SBM structure...
      node_list <- c(1:m_nodes)
      edge <- sample(node_list[-k], 15) - 1
      edge <- sort(edge)
      A[[k]] <- edge
    }
    
    system.time(alltimes <- sampleBlockHak(Time,A, Z, Mu, B, lam = 1))
    ### fit the models
    Pi = rep(1/K,K)
    B = matrix(runif(K*K),K,K)
    #diag(B) = rnorm(K,mean = 1, sd = 0.1)
    tau = matrix(runif(m_nodes*K),nrow=m_nodes,ncol=K)
    tau = tau/rowSums(tau)
    S = matrix(0,nrow = m_nodes,ncol = K)
    
    poisson_online <- estimate_Poisson(full_data = alltimes,
                     A,m_nodes,K,Time,dT = 5,
                     step_size = 0.5,
                     B,
                     tau,Pi,S,inter_T = 1,is_elbo = FALSE)
    
    est_Z <- apply(poisson_online$tau,1,which.max)
    
    
    test_events <- tibble(start = alltimes[,1],end = alltimes[,2],
                          Time = alltimes[,3])
    
    out <- bin_fun(test_events,m_nodes,max_Time = Time,window_size = 1)
    
    ### store the results
    # using sum of all events
    
    #results$sum[j] <- aricode::NMI(fcd::spectral.clustering(out[,,dim(out)[3]]),Z) 
    # using only events in final time window
    sum_adj <- apply(out, c(1,2), sum)#/dim(out)[3]
    results$SC_Sum[j] <- aricode::NMI(fcd::spectral.clustering(sum_adj,K=2),Z)
    # pz
    pz_est <- pz_estimator_3(out,time = Time,
                             l0 = l0,
                             m0 = m0,
                             m = m,
                             r= Time-1)
    results$SC_PZ[j] <- aricode::NMI(fcd::spectral.clustering(pz_est,K=2),Z)
    # our method
    results$OCD[j] <- aricode::NMI(Z,est_Z)
    results$nodes[j] <- m_nodes
    results$sim[j] <- j
    
  }
  all_results[[i]] <- results
  
}
```


```{r fig-agg-rank-pois}
all_results <- readRDS("../Sims/output_sims/agg_comp_pois.RDS")

n_sims <- 100
m_values <- c(50,100,250,500)

pois_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width=10) + 
  scale_x_continuous(breaks = m_values)

pois_agg

```



### Inhomogeneous Hawkes

We can also repeat this with an inhomogeneous Hawkes structure. Here aggregation
methods are unable to recover this community structure.

Note, our method seems to require knowing the window size to recover the
community structure.


```{r full agg sim, cache=TRUE, results='hide',message=FALSE,warning=FALSE,eval=FALSE,include=FALSE}
n_sims <- 20
m_values <- c(50,100,250,500)
no_methods <- 3 #ours, sum all, pz_estimator


l <- 2
m <- 2 # need m =l for final time point
m0 <- 1
l0 <- 2

# results <- matrix(NA, nrow = n_sims,ncol = no_methods+2 )
# 
# all_results <- list(m_50 = results, m_100 = results, m_200 = results,
#                     m_500 = results)
all_results <- list(rep(tibble(),length(m_values)))
#all_results <- tibble()

for(i in seq_along(m_values)) {
  m_nodes <- m_values[i]
  #results <- matrix(NA, nrow = n_sims,ncol = no_methods )
  # then each sim
  results <- matrix(NA, nrow = n_sims,ncol = no_methods+2 )
  colnames(results) <- c("SC_Sum","OCD","SC_PZ","nodes","sim")
  results <- as_tibble(results)
  for(j in 1:n_sims) {
    K <- 2
    H <- 2
    MuA <- array(0,c(K,K,H))
    MuA[,,1] <- matrix(c(0.8,0.2,0.6,0.4),2,2)
    MuA[,,2] <- matrix(c(0.4,0.7,0.2,0.7),2,2)
    
    Mu <- matrix(c(0.6,0.2,0.3,0.5),K,K,byrow = TRUE)
    B <- matrix(c(0.1,0.02,0.01,0.05),K,K,byrow = TRUE)
    Pi <- matrix(c(0.4,0.6),1,2)
    Z <- c(rep(0,m_nodes*Pi[1]),rep(1,m_nodes*Pi[2]))
    Time <- 100
    
    A <- list()
    for(k in 1:m_nodes){
      # could sample these here with SBM structure...
      node_list <- c(1:m_nodes)
      edge <- sample(node_list[-k], 20) - 1
      edge <- sort(edge)
      A[[k]] <- edge
    }
    
    system.time(alltimes <- sampleBlockHak_nonhomo(Time,
                                                   A, Z, MuA, B, 
                                                   window = 1, lam = 1))
    ### fit the models
    B_start <- matrix(runif(K*K),K,K)
    MuA_start <- array(runif(K*K*H),c(K,K,H))
    
    tau_start = matrix(runif(m_nodes*K),nrow=m_nodes,ncol=K)
    tau_start = tau_start/rowSums(tau_start)
    
    nonHawkes_online <- nonhomoHak_estimator_eff_revised(alltimes,
                                                         A,m_nodes,K,H,
                                                         window = 0.999,
                                                         T = Time,dT=10,
                                                         lam = 1.75,
                                                         gravity = 0.001,
                                                         B_start,
                                                         MuA_start,
                                                         tau_start)
    
    est_Z <- apply(nonHawkes_online$tau,1,which.max)
    
    
    test_events <- tibble(start = alltimes[,1],end = alltimes[,2],
                          Time = alltimes[,3])
    
    out <- bin_fun(test_events,m_nodes,max_Time = Time,window_size = 1)
    
    ### store the results
    # using sum of all events
    
    #results$sum[j] <- aricode::NMI(fcd::spectral.clustering(out[,,dim(out)[3]]),Z) 
    # using only events in final time window
    sum_adj <- apply(out, c(1,2), sum)#/dim(out)[3]
    results$SC_Sum[j] <- aricode::NMI(fcd::spectral.clustering(sum_adj,K=2),Z)
    # pz
    pz_est <- pz_estimator_3(out,time = Time,
                             l0 = l0,
                             m0 = m0,
                             m = m,
                             r= Time-1)
    results$SC_PZ[j] <- aricode::NMI(fcd::spectral.clustering(pz_est,K=2),Z)
    # our method
    results$OCD[j] <- aricode::NMI(Z,est_Z)
    results$nodes[j] <- m_nodes
    results$sim[j] <- j
    
  }
  all_results[[i]] <- results
  
}

```

```{r fig-agg-rank-inhawkes, message=FALSE,warning=FALSE}

all_results <- readRDS("../Sims/output_sims/agg_comp_inhom_hawk_wind_75.RDS")

inhaw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values)

inhaw_agg

```


## Hawkes

```{r}
all_results <- readRDS("../Sims/output_sims/agg_comp_hawk.RDS")

haw_agg <- all_results %>%
  bind_rows() %>%
  pivot_longer(cols=SC_Sum:SC_PZ, names_to = "Method") %>%
  group_by(nodes,Method) %>%
  summarise(NMI = mean(value), se = sd(value)/sqrt(n_sims)) %>%
  rowwise() %>%
  mutate(lower = max(NMI - se,0), upper = min(NMI + se,1)) %>%
  ggplot(aes(nodes,NMI,colour = Method)) + 
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper),width = 10) +
  scale_x_continuous(breaks = m_values)

haw_agg


```








This recovers the clusters with the correct window size...., which the binning
methods do not, even with the correct window size.


```{r, include=TRUE,echo=TRUE,eval=FALSE}
# using sum of all events
aricode::ARI(fcd::spectral.clustering(out[,,dim(out)[3]]),Z) 
# using only events in final time window
sum_adj <- apply(out, c(1,2), sum)/dim(out)[3]
aricode::ARI(fcd::spectral.clustering(sum_adj,K=2),Z)
# our method
aricode::ARI(Z,est_Z)

### considering just binary (so at least one event)
out_bin <- out
out_bin <- apply(out, c(2,3), function(x) as.numeric(x>0))
aricode::ARI(fcd::spectral.clustering(out_bin[,,dim(out_bin)[3]]),Z) 
# using only events in final time window
sum_adj_bin <- apply(out_bin, c(1,2), sum)/dim(out_bin)[3]
aricode::ARI(fcd::spectral.clustering(sum_adj,K=2),Z)
# our method
aricode::ARI(Z,est_Z)

```



## Parmaeter and Community Recovery


We then wish to establish the performance of our proposed estimation
procedure in terms of community recovery in simulation examples. To
do this we simulate data under each of the proposed block structures
and evaluate community recovery for this simulated data,
comparing to the known true cluster assignments.

```{r sim_homogeneous_Poisson, cache=TRUE}
Time = 500 # total time
K <- 3 # number of communities
Mu <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),
             K,K,byrow = TRUE) # block level rates between communities
B_Pois <- matrix(0,K,K,byrow = TRUE) # block level excitation, none here
m <- 100 # number of nodes
Pi <- matrix(c(0.4,0.3,0.3),1,3) # proportion of nodes in each community
Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3])) # assignment vector
# of nodes to communities


A <- list() # store edge structure
for(i in 1:m){
  # could sample these here with SBM structure...
  node_list <- c(1:m)
  edge <- sample(node_list[-i], 40) - 1
  edge <- sort(edge)
  A[[i]] <- edge
}

# then store these events
pois_events <- sampleBlockHak(Time, A, Z, 
                              Mu, B_Pois,lam = 1)


```

```{r fit_homogeneous_Poisson, echo=TRUE, results='hide',cache=TRUE}
Pi = rep(1/K,K)
B = matrix(runif(K*K),K,K)
diag(B) = rnorm(K,mean = 1, sd = 0.1)
tau = matrix(runif(m*K),nrow=m,ncol=K)
tau = tau/rowSums(tau)
S = matrix(0,nrow = m,ncol = K)

dT <- 5
inter_T <- 5

results_online <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.5,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = TRUE)

```

We can compare the estimated and true rate matrix.

```{r compare_Poisson_matrix}
cat("Estimated Poisson rate matrix \n")
results_online$B
cat("True Poisson rate matrix \n")
Mu

```

## Informal Convergence of Proposed Method

```{r informal_convergence_plots}
iter = seq(dT,Time,dT)
av_elbo = results_online$AveELBO
ll = results_online$logL
elbo = as_tibble(cbind(iter,elbo = as.vector(av_elbo),ll= as.vector(ll)))
plot_1 <- ggplot(data = elbo, aes(iter,av_elbo)) + 
  geom_line() + 
  xlab("Time") + ylab("Average ELBO") 

plot_2 <- ggplot(data = elbo, aes(iter,ll)) + 
  geom_line()+ylab("Average Log likelihood") + xlab("Time")

plot_1
plot_2

```

```{r community_recovery_metrics}
est_Z = apply(results_online$tau,1,which.max)
cat("Estimated Adjusted Rand Index:",
    mclust::adjustedRandIndex(Z,est_Z), "\n",
    sep = " ")
cat("Estimated Normalised Mutual Information:",
    aricode::NMI(Z,est_Z), "\n",
    sep = " ")

```
```{r online_nmi_plot}
time_points = dim(results_online$early_tau)[3]-1
early_nmi = rep(0,time_points)
for(i in 1:time_points){
  est_Z = apply(results_online$early_tau[,,i],1,which.max)
  early_nmi[i]= aricode::NMI(Z,est_Z)
}

online_nmi = cbind(iters = seq(from = dT*inter_T,by = dT*inter_T,length.out = time_points),early_nmi)

as_tibble(online_nmi) %>% ggplot(aes(iters,early_nmi)) + geom_line() +
  ylab("Online NMI") + xlab("Time")
```

Do we want to run multiple simulations and get the average of the plot across 
those? I think possibly...

## Rate of Convergence Simulations


As discussed in our paper, we provide results on the convergence 
rate in terms of step size. 

```{r convergence_single_step_size}
inter_B = results_online$inter_B
norm_B = apply(inter_B,c(3),sum)

## need to adjust this also for sampling size
b_recov = cbind(iters = seq(dT,500,dT),diff = abs(sum(Mu)-norm_B)/K^2)

as_tibble(b_recov) %>% 
  ggplot(aes(iters,diff)) + geom_line() + xlab("Time") +
  ylab("Difference in Rate Parameters") + ggtitle(latex2exp::TeX( 
    'Step size of $\\alpha = 0.5$') )
```

We also wish to compare these results for multiple values of $\alpha$,
in support of the theoretical results presented in our paper.

```{r sims_multiple_step_sizes,results='hide',cache=TRUE}
results_alpha_05 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.5,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)

results_alpha_025 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.25,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)
results_alpha_075 <- estimate_Poisson(full_data = pois_events,
                                   A,m,K,Time,dT,
                                   step_size = 0.75,
                                   B,
                                   tau,Pi,S,inter_T,is_elbo = FALSE)

```



```{r convergence_data}
inter_B_05 <- results_alpha_05$inter_B
rm(results_alpha_05)

inter_B_025 <- results_alpha_025$inter_B
rm(results_alpha_025)

inter_B_075 <- results_alpha_075$inter_B
rm(results_alpha_075)


norm_B_05 = apply(inter_B_05,c(3),sum)
b_recov_05 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_05)/K^2)


norm_B_025 = apply(inter_B_025,c(3),sum)
b_recov_025 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_025)/K^2)


norm_B_075 = apply(inter_B_075,c(3),sum)
b_recov_075 = cbind(iters = seq(dT,Time,dT),
                   diff = abs(sum(Mu)-norm_B_075)/K^2)

```


```{r summarise data}

recov <- bind_rows(as_tibble(b_recov_025),
                   as_tibble(b_recov_05)) %>%
  bind_rows(as_tibble(b_recov_075))

num_iters <- length(seq(dT,Time,dT))

recov$step_size <- c(rep("alpha_025",num_iters),
                     rep("alpha_05",num_iters),
                     rep("alpha_075",num_iters))
# saveRDS(recov, file = "step_sims")

```

```{r plot_rate for steps}
ggplot(data = recov,aes(x=iters,y=diff,))+
  geom_line(aes(colour=step_size)) + 
  # scale_color_manual(name = "Step Size")
  #                    #values=c("blue","red","black")) +
  xlab("Time") + ylab("Difference in rate matrix") +
  scale_color_discrete(labels = unname(latex2exp::TeX(c("$\\alpha = \ 0.25",
                                                        "$\\alpha = \ 0.50",
                                                        "$\\alpha = \ 0.75"))) ) +
  theme(legend.title = element_blank())

# recov %>% filter(step_size == "alpha_025") %>%
#   ggplot(aes(iters,diff)) + geom_line()

```



```{r load_old_data, include=FALSE,eval=FALSE}
eta025 <- readRDS("../analysis_old/Simulations/eta025.RDS")
eta050 <- readRDS("../analysis_old/Simulations/eta050.RDS")
eta075 <- readRDS("../analysis_old/Simulations/eta075.RDS")


eta_df = rbind(eta025,eta050)
eta_df = rbind(eta_df,eta075)
step = c(rep("eta025",200),rep("eta050",200),rep("eta075",200))
eta_df$step <- step
eta_df$step = as.factor(eta_df$step)


ggplot(data = eta_df,aes(x=iters,y=diff,))+
  geom_line(aes(colour=step)) + 
  scale_color_manual(name = "Step Size",values=c("blue","red","black"))


```



## Average NMI

We want to repeat the above plot where we examine the
cluster recovery over time, now considering multiple simulated
datasets. To do this we will run this here and cache the results.

```{r average_nmi_sim, cache=TRUE, results='hide'}
n_sims <- 100

# then run the sims here
Time <- 500
dT <- 5
inter_T <- 5
inters <- Time/(dT*inter_T)
sim_nmi = matrix(nrow=n_sims,ncol=inters)


for(iter in 1:n_sims) {
  print(iter)
  Time = 500
  #dT = 10
  K <- 3
  Mu <- matrix(c(0.6,0.2,0.3,0.1,1.0,0.4,0.5,0.4,0.8),K,K,byrow = TRUE)
  B_Pois <- matrix(0,K,K,byrow = TRUE)
  m <- 100
  Pi <- matrix(c(0.4,0.3,0.3),1,3)
  Z <- c(rep(0,m*Pi[1]),rep(1,m*Pi[2]),rep(2,m*Pi[3]))
  
  A <- list()
  for(i in 1:m){
    # could sample these here with SBM structure...
    edge_list <- c(1:m)
    edge <- sample(edge_list[-i], 40) - 1
    edge <- sort(edge)
    A[[i]] <- edge
  }
  
  system.time(alltimes <- sampleBlockHak(Time, A, Z, Mu, B_Pois, lam = 1))
  
  
  
  Pi = rep(1/K,K)
  B = matrix(runif(K*K),K,K)
  diag(B) = rnorm(3,mean = 1, sd = 0.1)
  tau = matrix(runif(m*K),nrow=m,ncol=K)
  tau = tau/rowSums(tau)
  S = matrix(0,nrow = m,ncol = K)
  results_online <- estimate_Poisson(full_data = alltimes,
                                     A,m,K,Time,dT,
                                     step_size = 0.5,
                                     B,
                                     tau,Pi,S,inter_T,is_elbo = FALSE)
  #print(results_online$Pi)
  time_points = dim(results_online$early_tau)[3]-1
  early_nmi = rep(0,time_points)
  for(i in 1:time_points){
    est_Z = apply(results_online$early_tau[,,i],1,which.max)
    early_nmi[i]= aricode::NMI(Z,est_Z)
  }
  
  #online_nmi = cbind(windows = seq(50,500,50),early_nmi)
  online_nmi = cbind(iters = seq(from = dT*inter_T,
                                 by = dT*inter_T,
                                 length.out = time_points),early_nmi)
  sim_nmi[iter,] = early_nmi
}


```


```{r average_nmi_plot}
new_sim_nmi = t(sim_nmi)
colnames(new_sim_nmi) = paste("Sim",c(1:n_sims),sep="")

nmi_online = as_tibble(new_sim_nmi)
nmi_online$Windows = seq(dT*inter_T,Time,dT*inter_T)


nmi_online %>% pivot_longer(
  cols = starts_with("Sim")) %>%
  group_by(Windows) %>%
  summarise(Ave = mean(value),sdev = sd(value)) %>%
  mutate(se = sdev/sqrt(50)) %>%
  ggplot(aes(Windows,Ave)) + geom_line() + geom_point() +
  geom_errorbar(aes(ymin=Ave-se, ymax=Ave+se), width=.1,color="blue") +
  ylab("Average NMI") + xlab("Time")

```



## Intensity Plots


