---
title: "Thesis Experiments"
author: "Owen G. Ward"
date: "4/18/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path = "thesis_figure/",
                      dev = "png",
                      fig.height = 5,
                      fig.width = 7)

library(here)
library(tidyverse)
source(here("Experiments/utils.R"))
```

# Figures for the Introduction


## Recovering community structure

```{r fig_1_exp_1}
fig_1_files <- list.files(path = here("Experiments/thesis_output/"),
                                      pattern = "fig_1_exp_1")

fig_1_data <- fig_1_files %>% 
  map_dfr(~readRDS(here("Experiments/thesis_output/", .x)))

fig_1_data %>% 
  group_by(Method) %>% 
  summarise(mean(ARI, na.rm = TRUE), sd(ARI, na.rm = TRUE))

# fig_1_data %>% 
#   ggplot(aes(Method, ARI)) +
#   geom_boxplot() +
#   facet_wrap(~Method, scales = "free_x")

### want to stratify by window size in PZ also
fig_1 <- fig_1_data %>% 
  mutate(new_x = ifelse(is.na(window_size), " ", window_size)) %>% 
  mutate(new_x = factor(new_x, levels = c(1:10, " "))) %>%
  mutate(Method = factor(Method, levels = c("Count", "PZ", "Point Process"))) %>% 
  ggplot(aes(new_x, ARI, fill = Method)) +
  geom_boxplot(alpha = 0.5) +
  facet_wrap(~Method, scales = "free_x") +
  labs(x = element_blank()) +
  theme(legend.position = "none",
        axis.title.y = element_text(size = 18),
        strip.text.x = element_text(size = 18))

fig_1
  
```



## Interpretable Clusters



# Simulation Experiments

## Community Recovery 


### Increasing the number of nodes

We first wish to confirm that we can recover communities when
data is simulated from this model. Here we show community recovery 
for a fixed $K=2, Time = 200$ setting where we increase the number of
nodes from $n=100,200,500,1000$.
With fixed sparsity $\rho = 0.1$. We do this using a fixed 
$n0$, the proportion of
time used for the initialization, 
and $m0$, the number of nodes used for the initialization.

```{r label, options}
## recovery as the number of nodes changes
node_sims <- list.files(path = here("Experiments", "thesis_output"),
                   pattern = "exp_pois_nodes_fixed")

node_data <- node_sims %>% 
  map_dfr(~readRDS(here("Experiments/thesis_output/", .x)))
  

node_data %>% 
  filter(init == "Init") %>% 
  group_by(nodes, n0, m0) %>% 
  summarise(mean(ARI), sd(ARI))

node_data %>% 
  # group_by(sim, nodes, m0) %>% top_n(1) %>% 
  filter(init == "Init") %>%
  # filter(n0 == 10, m0 == nodes/10) %>%
  ggplot(aes(as.factor(nodes), ARI)) +
  geom_boxplot() +
  labs(x = "Number of Nodes")

## this looks ok now I guess
```


### Increasing the number of communities


```{r pois_k, options}

## now just one setting for all, in terms of the sparse initialization 
## scheme

K_sims <- list.files(path = here("Experiments", "thesis_output"),
                   pattern = "exp_pois_K")

K_data <- K_sims %>% 
  map_dfr(~readRDS(here("Experiments/thesis_output/", .x)))

K_data %>% 
  filter(init == "Init") %>%
  ggplot(aes(as.factor(K), ARI)) +
  geom_boxplot() +
  ylim(c(0, 1)) +
  labs(x = "Number of Communities")

```

### Online Community Recovery


```{r online_ari}
on_sims <- list.files(path = here("Experiments", "thesis_output"),
                   pattern = "exp_pois_online")


online_dat <- on_sims %>% 
  map_dfr(~readRDS(here("Experiments/thesis_output/", .x)))

online_dat %>% 
  filter(m0 == nodes/4) %>%
  group_by(time, m0, nodes) %>% 
  summarise(Mean_ARI = mean(ARI), sd = sd(ARI)) %>% 
  mutate(lower = Mean_ARI - sd, upper = min(1, Mean_ARI + sd)) %>% 
  ggplot(aes(time, Mean_ARI, colour = as.factor(nodes))) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper), alpha = 0.4) +
  ylim(c(0, 1)) +
  labs(colour = "Number of Nodes", y = "ARI", x = "Time", width = 0.2) +
  # facet_wrap(~nodes) +
  NULL

## need this to be a bit tidier here, but fine


## Old Experiment 9 also has some interesting experiments
```

## Monitoring Convergence

Variational inference typically looks at the ELBO for this. 
We can also look at some sort of online version of the ELBO here, 
to evaluate convergence of our online estimation procedure.

```{r monitor_conv}

### look at a single sim here first
### use a loss sim here which stores the ELBO

loss_files <- list.files(path = here("Experiments/thesis_output/"), 
                          pattern = "exp_pois_online_loss_april_29")

## how is this normalised? by the cumulative number of events

### try get all of these estimates out at once


elbo_sims <- loss_files %>% 
  map(~read_rds(here("Experiments",
                     "thesis_output",
                     .x))) %>% 
  flatten() %>% 
  imap(~update_list(., sim = .y)) %>% 
  map_dfr(`[`, c("est_elbo", "sim", "Time")) %>% 
  mutate(est_elbo = as.vector(est_elbo)) %>% 
  group_by(sim) %>% mutate(ind = row_number() + 5)

elbo_sims %>% 
  ggplot(aes(ind, est_elbo)) +
  geom_line(aes(group = sim), alpha = 0.2) +
  facet_wrap(~Time, scales = "free") +
  labs(x = "Time", y = "ELBO")

```

## Parameter Recovery


To be added.

## Empirical Regret 

```{r exp_regret}
regret_files <- list.files(path = here("Experiments/thesis_output/"),
                          pattern = "exp_pois_regret_")

# regret_data <- regret_sims %>% 
#   map_dfr(~readRDS(here("Experiments/thesis_output/", .x)))

## look at individual sim



sim_50 <- read_rds(here("Experiments",
                             "thesis_output",
                             regret_files[3])) %>% 
  tibble(
  sim = 1:100,
  ARI = map_dbl(., "clust"),
  regret = map(., "regret"),
  dT = list(6:50)) %>% 
  unnest(cols = c(regret, dT)) %>% 
  select(sim, ARI, regret, dT) %>% 
  mutate(Time = 50)

sim_100 <- read_rds(here("Experiments",
                             "thesis_output",
                             regret_files[1])) %>% 
  tibble(
  sim = 1:100,
  ARI = map_dbl(., "clust"),
  regret = map(., "regret"),
  dT = list(6:100)) %>% 
  unnest(cols = c(regret, dT)) %>% 
  select(sim, ARI, regret, dT) %>% 
  mutate(Time = 100)



sim_200 <- read_rds(here("Experiments",
                             "thesis_output",
                             regret_files[2])) %>% 
  tibble(
  sim = 1:100,
  ARI = map_dbl(., "clust"),
  regret = map(., "regret"),
  dT = list(6:200)) %>% 
  unnest(cols = c(regret, dT)) %>% 
  select(sim, ARI, regret, dT) %>% 
  mutate(Time = 200)

sim_500 <- read_rds(here("Experiments",
                             "thesis_output",
                             regret_files[4])) %>% 
  tibble(
  sim = 1:100,
  ARI = map_dbl(., "clust"),
  regret = map(., "regret"),
  dT = list(6:500)) %>% 
  unnest(cols = c(regret, dT)) %>% 
  select(sim, ARI, regret, dT) %>% 
  mutate(Time = 500)

regret_sims <- bind_rows(sim_50,
                         sim_100,
                         sim_200,
                         sim_500)

regret_sims %>% 
  filter(ARI == 1) %>%
  ggplot(aes(dT, regret)) +
  geom_line(aes(group = sim), alpha = 0.2) +
  facet_wrap(~Time, scales = "free") +
  stat_function(fun = function(x) 2 * sqrt(x) * log(x*10000)^2,
                colour = "red") +
  labs(y = "Empirical Regret", x = "Time")

## over 90% converge, but not sure about this,
## am I scaling these correctly?

```

## Online Predictions

```{r online-loss}
loss_files <- list.files(path = here("Experiments/thesis_output/"), 
                          pattern = "exp_pois_online_loss_april_29")

loss_data <- loss_files %>% 
  map(~readRDS(here("Experiments/thesis_output/", .x))) %>% 
  flatten() %>% 
  imap(~update_list(., sim = .y)) %>% 
  map_dfr(`[`, c("online_loss", "batch_ave_loss", "sim", "clust")) %>% 
  reduce(data.frame) %>% 
  as_tibble() %>% 
  rename(Batch_Loss = elt, Sim = elt.1, ARI = elt.2) %>% 
  group_by(Sim) %>% 
  mutate(Time = max(dT)) 


batch_loss <- 
  loss_data %>% 
  group_by(Time) %>% 
  summarise(batch = unique(Batch_Loss))


loss_data %>% 
  # filter(Sim == 1) %>% 
  filter(Z == "Est_Z") %>% 
  # filter(Time == 200) %>% 
  ggplot(aes(dT, Loss)) +
  geom_line(aes(group = interaction(Sim, Z)), alpha = 0.2) +
  geom_hline(data = batch_loss, aes(yintercept =  batch), colour = "red") +
  facet_wrap(~Time, scales = "free_x") +
  theme(legend.position = "null")

# single_sim <- loss_data %>% 
#   reduce(data.frame) %>% 
#   as_tibble() %>% 
#   rename(Batch_Loss = elt, Sim = elt.1, ARI = elt.2) %>% 
#   filter(Sim == 1) 
# 
# batch_loss = single_sim$Batch_Loss[1]
# 
# single_sim %>% 
#   ggplot(aes(dT, Loss, colour = Z)) +
#   geom_line() +
#   geom_hline(yintercept = batch_loss)

### then just need to add the batch loss across the bottom

```


# Real Data Experiments

## Event Prediction
